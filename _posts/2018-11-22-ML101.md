---
layout: post
title: Machine Learning- 101
subtitle: ML-101 definitions in less than 15 minutes
---

#### Supervised and Unsupervised Learning
- __Supervised:__ All data is labeled (ground truth) and the algorithms learn to predict the output from the input data. Common methods are Naïve Bayes, Random Forest, Support Vector Machines.
- __Unsupervised:__ All data is unlabeled and the algorithms learn to inherent structure from the input data. Common methods are clustering and diemnsionality reduction.

####	Common Supervised Learning Methods
- __Naïve Bayes__
    A type of "probabilistic classifier" based on Bayes' theorem. It makes decisions using the conditional probability for each feature and prior probability of each class.
    It assumes the features are independent (a major limitation with real world data).
-	__Support Vector Machines__
    A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane (decision boundary).
    For non-linear classification, SVMs use the kernel trick by implicitly mapping their inputs into high-dimensional feature spaces using a kernel. Eg. Polynomial and exponential kernels.  
-	__K Nearest Neighbor__
    A simple classifier that uses a majority vote of the neighbors to classify a sample, i.e., a sample is assigned to the class that is most common among its k nearest neighbors.
-	__Decision Tree__
    Decision Tree Classifiers repetitively divide the feature space into sub parts by identifying separating lines repetitively. The criteria for forming separation lines is based on impurity (incorrectly classifying samples) or information gain (increase in information).
        
-	__Random Forest and Bagging & Boosting__
    > These are all ensemble methods.

    In *Bagging* (Bootstrap Aggregation), we create several subsets of data from training sample chosen randomly with replacement. Each collection of this subset data is used to train their decision trees. As a result, we end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree.

    *Random Forest* is an extension to bagging. It takes one extra step where in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees.

    *Boosting* creates a collection of predictors where learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree.

    *AdaBoost*: An example of boosting method. It focuses on classification problems and aims to combine a set of weak classifiers into a weighted sum to form a strong classifier.
    
#### Unsupervised Learning
-  __Clustering:__ The goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar.
    __K-means clustering:__ Cluster the data points into k groups by creating a centroid for each group. Method- define the k centroids randomly, find the closest centroid & update cluster assignments, move the centroids to the center of their clusters. Repeat.
    __Hierarchical Clustering:__ Build a hierarchy of clusters so there is flexibility in number of clusters. Method- start with N clusters (one for each sample), merge two clusters that are closest, recompute distances between clusters, repeat until you get one cluster of all samples, pick stage based on number of clusters.

-  __Dimensionality reduction__
    __Principal component analysis__ (PCA) is a statistical procedure that uses an orthogonal transformation to reproject the data into a new space that emphasizes variation. In this new space, highest variance of data lies on the first coordinate (first principal component), second highest variance on the second coordinate, and so on. The eigenvectors (principal components, linearly uncorrelated) determine the directions of the new feature space, and the eigenvalues determine their magnitude (contribution).
    
    __Singular value decomposition__ (SVD) is a computation that allows us to decompose that big matrix into a product of 3 smaller matrices. ``` M = U∑V```The values in the r*r diagonal matrix Σ are called singular values that can be used to compress the original matrix.

#### Classification and Regression
- A classification problem is when the output variable is a category. A regression problem is when the output variable is a continuous real value.

-	__Linear Regression__
    Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data (to predict a continuous output). The most common method for fitting a regression line is the method of least-squares. The best fit line is the one for which total prediction error (all data points) are as small as possible. Error is the distance between the point to the regression line (residuals are normal distributed). Solved using Normal Equation (direct) solution or gradient descent (iterative).
    
-	__Logistic Regression__
    It is a binary classification algorithm which means that here there will be discreet valued output for the function. The hypothesis function is a Sigmoid function: s(z) = 1 / (1 + e^-z). It uses Maximum Likelihood estimation to get a solution.
    Since the prediction function is non-linear (sigmoid), squaring it like in MSE results in a non-convex function with many local minimums. Instead, we use a cost function called Cross-Entropy, also known as ```Log Loss = −(ylog(p)+(1−y)log(1−p))```.
    Derivative of Sigmoid: ```s′(z)=s(z)(1−s(z))```
    Derivative of cost with respect to weights: ```C′=x(s(z)−y)``` (gradient is the same as the MSE (L2) gradient).

#### Gradient Descent
- Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Gradient descent will converge into global minimum only if the function is convex. The size of the steps is called the learning rate.
```    W_new = W_old - alpha * delta_W ```
    
    
-  __Cross Entropy or Log Loss:__ It measures the performance of a classification model whose output is a probability value between 0 and 1.
    The benefits of taking the logarithm is that for both y=1 and y=0, cost functions are smooth monotonic (always increasing or always decreasing) that makes it easy to calculate the gradient and minimize cost. The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions. Multiclass classification: ```−∑ y_c.log(p_c)```

-  __Hinge Loss:__ It is a loss function used for training classifiers. It is used for "maximum-margin" classification, most notably for SVMs). It is convex but is not differentiable.


#### Learning Paradigms
-	__Train, validation and test set__
    *Training Dataset:* The sample of data used to fit the model.
    *Validation Dataset:* The sample of data used to provide an unbiased evaluation of a model fit on the training dataset for tuning hyperparameters.
    *Test Dataset:* The sample of data used to provide an unbiased evaluation of a final model.
    *Cross-validation:* It is a resampling procedure used to evaluate machine learning models on a limited data sample. In K Fold cross validation, the data is divided into K subsets.

-	__Regularization__
    Regularization is a technique used to avoid this overfitting problem. It adds a penalty on the different parameters of the model to reduce the freedom of the model (bigger loss for more complex models).
    
    __L2__ regularization (Ridge) adds a penalty equal to the sum of the squared value of the coefficients. L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero). Derivations of the L2 norm are easily computed and is easy to use gradient based learning methods. 
    
    __L1__ regularization (Lasso) adds a penalty equal to the sum of the absolute value of the coefficients. It tends to drive some weights to exactly zero (introducing sparsity in the model for feature selection). L1 is not sensitive to outliers since it optimizes the median. 
    
    > Regularization, significantly reduces the variance of the model, without substantial increase in its bias.
    
    __Bias-variance tradeoff:__ It is a tradeoff between a model's ability to minimize bias and variance.

    *Bias* is the difference between the average prediction of our model and the correct value. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data. (Underfit)
    *Variance* is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the test data. (Overfit)
    
-	__Performance Measurement__
    __Accuracy:__ It is the ratio of number of correct predictions to the total number of input samples.
    __Confusion Matrix:__ (error matrix) is a table to report performance of a classifier with axis as actual and predicted on a per class basis.
    __Precision:__ It is the number of correct positive results divided by the number of positive results predicted by the classifier. Used when False Positive not wanted in problem. ```(TP/(TP+FP))```
    __Recall:__ It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). Used when False Negative not wanted in problem. ```(TP/(TP+FN))```
    __F1:__ F1 Score is the Harmonic Mean between precision and recall. Better than accuracy for imbalanced datasets.
    __True Positive Rate (Sensitivity):__ True Positive Rate is defined as ```TP/ (FN+TP)```. True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.
    __False Positive Rate (Specificity):__ False Positive Rate is defined as ```FP / (FP+TN)```. False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.
    __Area Under Curve:__ It is one of the most widely used metrics for evaluation for binary classification problem. AUC is the area under the curve of plot False Positive Rate (x) vs True Positive Rate (y). AUC of a classifier is the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example.
    
- __One vs All__
    One-vs-all classification is a method which involves training N distinct binary classifiers, each designed for recognizing a particular class. Then those N classifiers are collectively used for multi-class classification. Another approach is one-vs-one where we train 2 classes at a time and repeat this for all the two class combinations.    
